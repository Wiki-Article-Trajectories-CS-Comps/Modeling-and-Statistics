# -*- coding: utf-8 -*-
"""Keystone Pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F1BPtVG439BIiurWtoraAxfQW6VxqkOb

# Helper Functions and Imports
run everything in this section to load all necessary functions
"""

import matplotlib.pyplot as plt
import statistics
import pandas as pd
import pickle
import statsmodels.api as sm
from statsmodels.formula.api import ols
import seaborn as sns
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import make_scorer
from sklearn.metrics import accuracy_score
import numpy as np
!pip install mord
import sys

from google.colab import drive
drive.mount('/content/drive')

def openDFWithConfounding(filepath):
    with open('/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /' + filepath, 'rb') as f:
      df = pickle.load(f)
    with open('/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /data/df_confounding.pkl', 'rb') as f:
      df_confounding = pickle.load(f)
      cols = df_confounding.columns.tolist()
      cols = cols[1:] + cols[:1]
      df_confounding = df_confounding[cols]
    try:
        return df.join(df_confounding)
    except ValueError:
        return df
    # return df.join(df_confounding)

def getIndependentVars(df, depVar, columns):
    if columns != 'all':
        return df.drop(depVar, axis=1).iloc[:, columns]
    return df.drop(depVar, axis=1)

from sklearn.linear_model import LinearRegression, LogisticRegression
from mord import LogisticAT, LogisticIT
from sklearn import preprocessing

# instantiate models
model_linear = LinearRegression()
model_1vR = LogisticRegression(multi_class='ovr',
    class_weight='balanced')
model_multi = LogisticRegression(multi_class='multinomial',
    solver='lbfgs',
    class_weight='balanced', max_iter=760)
model_ordinal_IT = LogisticIT()
model_ordinal = LogisticAT(alpha=0)  # alpha parameter set to zero to perform no regularisation
models = [model_linear, model_1vR, model_multi, model_ordinal, model_ordinal_IT]
models_str = ["Linear Regression", "Logistic Regression (one vs. rest)",
              "Logistic Regression (multinomial)", "Ordered Logistic Regression AT", "Ordered Logistic Regression IT"]

# instantiate preprocessing tools
scaler = preprocessing.StandardScaler()

def trim_correlated(df_in, threshold, dependent_var):
    df_corr = df_in.corr(method='pearson', min_periods=1)
    df_not_correlated = ~(df_corr.mask(np.tril(np.ones([len(df_corr)]*2, dtype=bool))).abs() > threshold).any()
    un_corr_idx = df_not_correlated.loc[df_not_correlated[df_not_correlated.index] == True].index
    df_out = df_in[un_corr_idx]
    print("Uncorrelated independent variables:")
    print(*df_out.columns, sep=', ')
    return df_out.join(dependent_var)

def get_model_scoring(features, target, model_type, scoring_list=[], folds=5):
    def acc_fun(target_true, target_fit):
        target_fit = np.round(target_fit)
        target_fit.astype('int')
        return accuracy_score(target_true, target_fit)

    scoring_list = [make_scorer(i) for i in [mean_absolute_error, acc_fun] + scoring_list]
    scoring_map = ["LR", "1VR", "MLR", "OLR"]

    if model_type == "ALL":
        model_scorings = [[cross_val_score(model, features, target, cv=folds,
                                       scoring=scoring_i) for scoring_i in scoring_list] for model in models]
        return model_scorings, [[np.mean(scoring_array) for scoring_array in scoring_type] for scoring_type in model_scorings]

    model_scorings = [cross_val_score(models[scoring_map.index(model_type)], features, target, cv=folds,
                                       scoring=scoring_i) for scoring_i in scoring_list]
    return model_scorings, [np.mean(scoring_array) for scoring_array in model_scorings]

# scoring_list is a list of tuples (scoring_func, quality order)
# where the first element in the tuple is the 
def run_models(independent_vars, dependent_vars, model_type, scoring_list=[], folds=5):

    def scoring_type(x):
        if x == 0:
            return "Mean Absolute Error"
        elif x == 1:
            return "Accuracy"
        else:
            return "Scoring " + str(x + 3)

    def quality_order(x):
        if x == 0 or 1:
            return x 
        else:
            return [i[1] for i in scoring_list][x]

    model_scorings, scoring_vals = get_model_scoring(independent_vars, dependent_vars, 
                                       model_type, [i[0] for i in scoring_list if scoring_list], folds)
    
    for i in range(len(scoring_list) + 2):
        sorted_list = sorted([(models_str[j], scoring_vals[j][i]) for j in range(len(scoring_vals))], 
                                      key=lambda tup: tup[1], 
                                      reverse=quality_order(i))
        # sorted_list = [elm for elm in sorted_list]
        print(scoring_type(i) + ':')
        for j in sorted_list:
            print('\t', j)
        # print("%(scoring)s:\n %(sorted_list)s" % 
        #       {"scoring": scoring_type(i), 
        #         "sorted_list": print(*sorted_list, sep='\n')})
    return model_scorings

def numeric_rankings(df):
    class_mapping = {'FA': 1, 'GA': 2, 'B': 3, 'C': 4, 'ST': 5, 'SB': 6}
    df = df.apply(lambda x: class_mapping[x])
    return df

"""## KNN ML Stuff"""

from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import train_test_split 
from sklearn.neighbors import KNeighborsClassifier 
scaler = StandardScaler() 
from sklearn.metrics import classification_report, confusion_matrix

def runKNN(df, k = 1, base=True):
  scaler.fit(df.drop('class', axis = 1)) 
  scaled_features = scaler.transform(df.drop('class', axis = 1)) 
  
  df_feat = pd.DataFrame(scaled_features, columns = df.columns[:-1]) 
  # return df_feat
  # print(df_feat.head())
    
  X_train, X_test, y_train, y_test = train_test_split( 
        scaled_features, df['class'], test_size = 0.30) 
    
  # Trying to come up with a model to predict whether someone will predict class or not. 
  # We'll start with k = 1. 
    
    
  knn = KNeighborsClassifier(n_neighbors = k) 
    
  knn.fit(X_train, y_train) 
  pred = knn.predict(X_test) 
    
  # Predictions and Evaluations 
  # Let's evaluate our KNN model!  
  print(confusion_matrix(y_test, pred)) 
    
  print(classification_report(y_test, pred))
  # return classification_report(y_test, pred)
  if not base:
    error_rate = [] 
  
    for i in range(1, 70): 
          
        knn = KNeighborsClassifier(n_neighbors = i) 
        knn.fit(X_train, y_train) 
        pred_i = knn.predict(X_test) 
        error_rate.append(np.mean(pred_i != y_test)) 
      
    plt.figure(figsize =(10, 6)) 
    plt.plot(range(1, 70), error_rate, color ='blue', 
                    linestyle ='dashed', marker ='o', 
            markerfacecolor ='red', markersize = 10) 
      
    plt.title('Error Rate vs. K Value') 
    plt.xlabel('K') 
    plt.ylabel('Error Rate')

openDFWithConfounding(filepath='data/df_undirected_stats12Con.pkl')

runKNN(openDFWithConfounding(filepath='data/df_undirected_stats12Con.pkl'), base = False)

max_acc = 0
best_k = 0
for i in range(23, 50):
  for j in range(100):
    knn = runKNN(openDFWithConfounding(filepath='data/df_undirected_stats12Con.pkl'), i)
    stats = [elm for elm in knn.split(' ') if len(elm) > 0]
    max_acc = float(stats[stats.index("accuracy") + 1]) if float(stats[stats.index("accuracy") + 1]) > max_acc else max_acc
    best_k = i

max_acc

best_k

float([elm for elm in knn.split(' ') if len(elm) > 0][[elm for elm in knn.split(' ') if len(elm) > 0].index('accuracy')+1])

"""# Ridge and Lasso"""

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
ridge = Ridge(normalize=True)

def run_ridge(df):
  X = df.drop(['class'], axis = 1)
  y = df['class'].values.reshape(-1,1)
  parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]}
  ridge_regressor = GridSearchCV(ridge, parameters, scoring="accuracy", cv=10)
  ridge_regressor.fit(X, y)
  print(ridge_regressor.best_params_)
  print(ridge_regressor.best_score_)

df_temp = openDFWithConfounding('data/df_undirected_stats12Con.pkl')
df_temp['class'] = numeric_rankings(df_temp['class'])

independent_vars = getIndependentVars(df, 'class', 'all')
df = pd.DataFrame(scaler.fit_transform(independent_vars), columns=independent_vars.columns)

df['class'] = pd.DataFrame(scaler.fit_transform(df_temp[['class']]), columns=["class"])

run_ridge(df)

"""# Model Outputs
set file path and independent variables and run every cell to get scores
"""

# filepath starts from the shared drive
# returns the join of stats with confounding stats

df = openDFWithConfounding(filepath='data/df_directed_stats12Con.pkl')

with open('/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /data/df_directed_normalized_ecc.pkl', 'rb') as f:
  df_ecc = pickle.load(f)
df = df.join(df_ecc)

with open('/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /data/df_directed_normalized_m.pkl', 'rb') as f:
  df_m = pickle.load(f)
df = df.join(df_m)

with open('/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /data/df_directed_12con_normalized.pkl', 'wb') as f:
  pickle.dump(df, f)

compression_opts = dict(method='zip', archive_name='/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /data/df_directed_12con_normalized.csv')  
df.to_csv('/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /data/df_directed_12con_normalized.zip', index=False, compression=compression_opts)  

df

# with open('/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /' + 'data/df_undirected_stats_connectedness.pkl', 'rb') as f:
#   df_con = pickle.load(f)
# df = df.join(df_con)

# with open('/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /' + 'data/df_undirected_density_m.pkl', 'rb') as f:
  # df2 = pickle.load(f)

# df = df.join(df2)
# df
#df = df[["article size", "class"]]

#with open('/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /data/CSV (for R)/directed_stats12ConNoCor.csv', 'w') as f:
#    f.write(df.to_csv())

#with open('/content/drive/Shared drives/CS Comps: Wikipedia Article Trajectories /data/df_undirected_density_m.pkl', 'wb') as f:
#  pickle.dump(df2, f)

# change dependent variable name to whichever column is the dependent variable
depVarName='class'
# to subset specific independent vars set value to list of columns
input_columns='all'

dependent_var = df[[depVarName]].reset_index(drop=True)
independent_vars = getIndependentVars(df, depVarName, input_columns)

# scale the columns in the dataframe
df = pd.DataFrame(scaler.fit_transform(independent_vars), columns=independent_vars.columns)
df

df.corr()

correlation_threshold = 0.9
df_uncor = trim_correlated(df, correlation_threshold, dependent_var)

df_uncor

# independent variables are s
def putting_it_all_together(df):
    try:
      return run_models(df[[name for name in df.columns if name != depVarName]], numeric_rankings(df[depVarName]), "ALL")
    except KeyError:
      df['class'] = dependent_var
      return run_models(df[[name for name in df.columns if name != depVarName]], numeric_rankings(df[depVarName]), "ALL")

model_outputs = putting_it_all_together(df_uncor)

model_outputs = putting_it_all_together(df[['article size', 'diameter', 'betweenness', 'density']])

model_outputs = putting_it_all_together(df[['article size', 'diameter', 'betweenness', 'density']])

model_outputs = putting_it_all_together(df[['article size', 'diameter', 'betweenness', 'density', 'closeness']])

model_outputs = putting_it_all_together(df[['betweenness', 'global clustering', 'node connectivity', 'density', 'clustering', 'diameter', 'avg eccentricity', 'm', 'edge connectivity']])

model_outputs = putting_it_all_together(df[['diameter', 'betweenness', 'density', 'closeness', 'clustering']])

"""# v i s u a l i z e"""

# set title of graph
title = "Model Performance"

def nuss_style_fun(ax, title):
    
    #remove top and right frame parts
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    
    # set left and bottom axis to grey
    ax.spines['left'].set_color('grey')
    ax.spines['bottom'].set_color('grey')
    
    # set ticks to grey
    ax.tick_params(axis='x', colors='grey')
    ax.tick_params(axis='y', colors='grey')
    
    #set labels to grey
    ax.yaxis.label.set_color('grey')
    ax.xaxis.label.set_color('grey') 
    
    # align axis labels with axis ends
    ax.set_xlabel(xlabel=None,
                  position=[0, 0],
                  horizontalalignment='left',
                  color='grey',
                  size=14)
    ax.set_ylabel(ylabel=None,
                  position=[0, 1],
                  horizontalalignment='right',
                  color='grey',
                  size=14)
    
    #align title
    ax.set_title(label=title,
                 loc='left',
                 color=(0.41, 0.41, 0.41),
                 size=16)
    
    return ax

# Commented out IPython magic to ensure Python compatibility.
def plot_metrics(MAE, acc, title):
#     %matplotlib inline

    fig, axs = plt.subplots(ncols=2, figsize=[10, 6])
    fig.suptitle(title + '\n\n',
                color='dimgrey',
                 size=22)

    axs[0] = nuss_style_fun(ax=axs[0], title='\n\nMagnitude prediction')

    sns.boxplot(y=['Linear Regression', 'Logistic Regression\n(one versus rest)', 
                   'Logistic regression\n(multinomial)', 'Ordered logistic regression'],
                x=MAE, # [MAE_linear, MAE_1vR, MAE_multi, MAE_ordinal], 
                ax=axs[0])
    axs[0].set(xlabel='Mean absolute error (lower is better)',
          ylabel=' ')

    axs[1] = nuss_style_fun(ax=axs[1], title='\n\nCategory prediction')

    sns.boxplot(y=['Linear Regression', 'Logistic Regression\n(one versus rest)', 
                   'Logistic regression\n(multinomial)', 'Ordered logistic regression'],
                x=acc, # [acc_linear, acc_1vR, acc_multi, acc_ordinal], 
                ax=axs[1])
    axs[1].set(xlabel='Classification accuracy (higher is better)',
          ylabel=' ',
          xlim=[0, 1])
    axs[1].get_yaxis().set_ticks([])

    fig.tight_layout()

plot_metrics([mae[0] for mae in model_outputs], [acc[1] for acc in model_outputs], title)

"""# Test Code
This is just some garbage don't look here
"""

from random import randint 
def some_scoring(x):
    if x == 0:
      return "yes"
    if x == 1:
      return "no"

# nested_list = [[randint(0,9) for j in range(3)] for i in range(5)]
for i in range(3):
    print("Scoring %(number)s: %(sl)s" % 
          {"number": some_scoring(i), "sl": sorted([("index " + str(j), nested_list[j][i]) for j in range(len(nested_list))], key=lambda tup: tup[1])})

print("Hello\b p")
nested_list

get_model_scoring(directed_features, directed_target, "ALL")[1]

# import statsmodels.formula.api as sm
# logistic1 = sm.logit(formula='radius~density', data=df)
# fitted1 = logistic1.fit()
# fitted1.summary()
# from sklearn.model_selection import cross_val_predict
# from sklearn.metrics import confusion_matrix
# y_pred = cross_val_predict(model_multi, df, numeric_rankings(dependent_var['class']), cv=5)
# confusion_matrix(numeric_rankings(dependent_var['class']), y_pred).ravel()
# from sklearn.metrics import confusion_matrix
# y_true = [0, 0, 0, 1, 1, 1, 1, 1]
# y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
# tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
# specificity = tn / (tn+fp)
# specificity

